install.packages("corrplot")
install.packages("visreg")
install.packages("rgl")
install.packages("scatterplot3d")
install.packages("car")
install.packages("knitr")

library(car)
library(corrplot) 
library(visreg) 
library(rgl)
library(knitr)
library(scatterplot3d)


#   In this example we???ll extend the concept of 
#   linear regression to include multiple predictors. 
#   Prestige will be our dataset of choice 
#   and can be found in the car package library(car).

# Inspect and summarize the data.
head(Prestige,5)

str(Prestige)

summary(Prestige)

#   the Prestige dataset is a data frame with 
#   102 rows and 6 columns. Each row is an 
#   observations that relate to an occupation. 
#   The columns relate to predictors such as 
#   average years of education, percentage of 
#   women in the occupation, prestige of the occupation, etc.


#   For our multiple linear regression example, we???ll use 
#   more than one predictor. Our response variable will 
#   be Income and we will include women, 
#   prestige and education as our list of predictor variables. 
#   Remember that Education refers to the average number of years 
#   of education that exists in each profession. The women 
#   variable refers to the percentage of women in the 
#   profession and the prestige variable refers to a 
#   prestige score for each occupation 
#   (given by a metric called Pineo-Porter), 
#   from a social survey conducted in the mid-1960s.



# Let's subset the data to capture income, education, women and prestige.
newdata <- Prestige[,c(1:4)]
summary(newdata)
str(newdata)


#   Our new dataset contains the four variables 
#   to be used in our model. It is now easy for 
#   us to plot them using the plot function:


# Plot matrix of all variables.
plot(newdata, 
     pch=16, col="blue", 
     main="Matrix Scatterplot of Income, Education, Women and Prestige")

#   The matrix plot above allows us to vizualise the 
#   relationship among all variables in one single image. 
#   For example, we can see how income and education are 
#   related (see first column, second row top to bottom graph).


#   Another interesting example is the relationship between 
#   income and percentage of women (third column left to 
#   right second row top to bottom graph). Here we can see 
#   that as the percentage of women increases, average 
#   income in the profession declines.


#   Also from the matrix plot, note how prestige seems 
#   to have a similar pattern relative to education 
#   when plotted against income (fourth column left 
#   to right second row top to bottom graph).


#   To keep within the objectives of this study example, 
#   we???ll start by fitting a linear regression on this 
#   dataset and see how well it models the observed data. 
#   We???ll add all other predictors and give each of them 
#   a separate slope coefficient. We want to estimate 
#   the relationship and fit a plane (note that in a 
#   multi-dimensional setting, with two or more predictors 
#   and one response, the least squares regression line 
#   becomes a plane) that explains this relationship.



#   The model will estimate the value of the intercept 
#   (B0) and each predictor???s slope (B1) for education, 
#   (B2) for prestige and (B3) for women. The intercept 
#   is the average expected income value for the average 
#   value across all predictors. The value for each slope 
#   estimate will be the average increase in income 
#   associated with a one-unit increase in each predictor 
#   value, holding the others constant. We want our model 
#   to fit a line or plane across the observed relationship 
#   in a way that the line/plane created is as close as 
#   possible to all data points.



#   Let???s start by using R lm function. 
#   The lm function is used to fit linear models. 
#   For more details, 
#   see: https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html. 
#   Here we are using Least Squares approach again.



set.seed(1)

# Center predictors.
education.c <- scale(newdata$education, center=TRUE, scale=FALSE)
prestige.c <- scale(newdata$prestige, center=TRUE, scale=FALSE)
women.c <- scale(newdata$women, center=TRUE, scale=FALSE)

# bind these new variables into newdata and display a summary.
new.c.vars <- cbind(education.c, prestige.c, women.c)
newdata <- cbind(newdata, new.c.vars)
names(newdata)[5:7] <- c("education.c", "prestige.c", "women.c" )
summary(newdata)



#   note we created a centered version of 
#   all predictor variables each ending with a .c 
#   in their names. These new variables were 
#   centered on their mean. This transformation 
#   was applied on each variable so we could 
#   have a meaningful interpretation of the 
#   intercept estimates. Centering allows us 
#   to say that the estimated mean income is $6,798 
#   when we consider the average number of years 
#   of education, the average percent of women 
#   and the average prestige from the dataset.



# fit a linear model and run a summary of its results.
mod1 <- lm(income ~ education.c + prestige.c + women.c, data=newdata)
summary(mod1)

#   From the model output and the scatterplot 
#   we can make some interesting observations:

#####   Observation - 1 

#   For any given level of education and prestige in a profession, 
#   improving one percentage point of women in a given profession 
#   will see the average income decline by $-50.9. Similarly, 
#   for any given level of education and percent of women, 
#   seeing an improvement in prestige by one point in a given 
#   profession will lead to an an extra $141.4 in average income.

#####   Observation - 2

#   Note also our Adjusted R-squared value (we???re now looking 
#   at adjusted R-square as a more appropriate metric of 
#   variability as the adjusted R-squared increases only if 
#   the new term added ends up improving the model more than 
#   would be expected by chance). In this model, we arrived 
#   in a large R-squared number of 0.6322843.

### - Activity - Work out a Simple Linear Rgression Model
###   with Income as response variable and only Education.c
###   as predictor variable


#####   Observation - 3

#   Recall from our previous simple linear regression activity 
#   that our centered education predictor variable had a 
#   significant p-value (close to zero). But from the multiple 
#   regression model output above, education no longer displays 
#   a significant p-value. Here, education represents the 
#   average effect while holding the other variables women and 
#   prestige constant. From the matrix scatterplot seen earlier, 
#   we can see the pattern income takes when regressed on 
#   education and prestige. Note how closely aligned their 
#   pattern is with each other. So in essence, when they are 
#   put together in the model, education is no longer significant 
#   after adjusting for prestige. When we have two or more 
#   predictor variables strongly correlated, we face a problem 
#   of collinearity (the predictors are collinear).



#   Let???s validate this situation with a correlation plot:

# Plot a correlation graph
newdatacor <- cor(newdata[1:4])
corrplot(newdatacor, method = "number")


#   The correlation matrix shown above highlights the 
#   situation we encoutered with the model output. 
#   Notice that the correlation between education and 
#   prestige is very high at 0.85. This reveals each 
#   profession???s level of education is strongly aligned 
#   to each profession???s level of prestige. So in essence, 
#   education???s high p-value indicates that women and 
#   prestige are related to income, but there is no evidence 
#   that education is associated with income, at least not 
#   when these other two predictors are also considered in the model.


#   The model output can also help answer whether there is a 
#   relationship between the response and the predictors used. 
#   We can use the value of our F-Statistic to test whether all 
#   our coefficients are equal to zero (testing for the null 
#   hypothesis which means). The F-Statistic value from our model 
#   is 58.89 on 3 and 98 degrees of freedom. So assuming that 
#   the number of data points is appropriate and given that the 
#   p-values returned are low, we have some evidence that at least 
#   one of the predictors is associated with income.


#   Given that we have indications that at least one of the 
#   predictors is associated with income, and based on the 
#   fact that education here has a high p-value, we can consider 
#   removing education from the model and see how the model fit changes.


# fit a linear model excluding the variable education
mod2 = lm(income ~ prestige.c + women.c, data=newdata)
summary(mod2)



#   The model excluding education has in fact improved our 
#   F-Statistic from 58.89 to 87.98 but no substantial improvement 
#   was achieved in residual standard error and adjusted R-square value. 
#   This is possibly due to the presence of outlier points in the data.



#   Let???s plot this last model???s residuals:


# Plot model residuals.
plot(mod2, pch=16, which = 1)


#   Note how the residuals plot of this last model shows some 
#   important points still lying far away from the middle 
#   area of the graph.


#   Let???s visualize a three-dimensional interactive graph 
#   with both predictors and the target variable and the 
#   linear fit from the last model:


newdat <- expand.grid(prestige.c=seq(-35,45,by=5),
                      women.c=seq(-25,70,by=5))
newdat$pp <- predict(mod2,newdata=newdat)
with(newdata,plot3d(prestige.c,
                    women.c,
                    income, 
                    col="blue", 
                    size=1, 
                    type="s", 
                    main="3D Linear Model Fit"))
with(newdat,surface3d(unique(prestige.c),
                      unique(women.c),
                      pp,
                      alpha=0.3,
                      front="line", 
                      back="line"))


#   Note from the 3D graph above 
#   (you can interact with the plot by cicking 
#   and dragging its surface around to change 
#   the viewing angle) how this view more clearly 
#   highlights the pattern existent across prestige 
#   and women relative to income. Also, this interactive 
#   view allows us to more clearly see those three or 
#   four outlier points as well as how well our 
#   last linear model fit the data.


#   At this stage we could try a few different 
#   transformations on both the predictors and 
#   the response variable to see how this would 
#   improve the model fit. For now, let???s apply a 
#   logarithmic transformation with the log function 
#   on the income variable (the log function here 
#   transforms using the natural log. If base 10 is 
#   desired log10 is the function to be used). 
#   Also, we could try to square both predictors. 
#   Let???s apply these suggested transformations directly 
#   into the model function and see what happens with 
#   both the model fit and the model accuracy.


# fit a model excluding the variable education,  log the income variable.
mod3 = lm(log(income) ~ prestige.c 
          + I(prestige.c^2) 
          + women.c 
          + I(women.c^2) , 
          data=newdata)
summary(mod3)


# Plot model residuals.
plot(mod3, pch=16, which=1)


# Now, the 3D Model
newdat2 <- expand.grid(prestige.c=seq(-35,45,by=5),
                       women.c=seq(-25,70,
                                   by=5))
newdat2$pp <- predict(mod3,newdata=newdat2)
with(newdata,plot3d(prestige.c,
                    women.c,
                    log(income), 
                    col="blue", 
                    size=1, 
                    type="s", 
                    main="3D Quadratic Model Fit with Log of Income"))
with(newdat2,surface3d(unique(prestige.c),
                       unique(women.c),
                       pp,
                       alpha=0.3,
                       front="line", 
                       back="line"))



#   By transforming both the predictors and the target 
#   variable, we achieve an improved model fit. 
#   Note how the adjusted R-square has jumped to 
#   0.7545965. Most predictors??? p-values are significant. 
#   Here, the squared women.c predictor yields a weak 
#   p-value (maybe an indication that in the presence 
#   of other predictors, it is not relevant to include 
#   and we could exclude it from the model.)


#   Let???s go on and remove the squared women.c 
#   variable from the model to see how it changes:


# fit a model excluding the variable education,  
# log the income variable.
mod4 = lm(log(income) ~ prestige.c 
          + I(prestige.c^2) 
          + women.c , 
          data=newdata)
summary(mod4)


# Plot model residuals.
plot(mod4, pch=16, which=1)


# Plot 3D Model
newdat3 <- expand.grid(prestige.c=seq(-35,45,
                                      by=5),
                       women.c=seq(-25,70,
                                   by=5))
newdat3$pp <- predict(mod4,newdata=newdat3)
with(newdata,plot3d(prestige.c,
                    women.c,
                    log(income), 
                    col="blue", 
                    size=1, 
                    type="s", 
                    main="3D Quadratic Model Fit with Log of Income excl. Women^2"))
with(newdat3,surface3d(unique(prestige.c),
                       unique(women.c),
                       pp,
                       alpha=0.3,
                       front="line", 
                       back="line"))



#   Note now that this updated model yields a 
#   much better R-square measure of 0.7490565, 
#   with all predictor p-values highly significant 
#   and improved F-Statistic value (101.5). 
#   The residuals plot also shows a randomly 
#   scattered plot indicating a relatively good fit 
#   given the transformations applied due to the 
#   non-linearity nature of the data.


#   In summary, we???ve seen a few different multiple 
#   linear regression models applied to the Prestige 
#   dataset. We tried an linear approach. 
#   We created a correlation matrix to understand 
#   how each variable was correlated. Subsequently, 
#   we transformed the variables to see the effect 
#   in the model. We???ve created three-dimensional 
#   plots to visualize the relationship of the 
#   variables and how the model was fitting 
#   the data in hand.


















